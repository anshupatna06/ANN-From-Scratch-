# ANN-From-Scratch-
"DL models implemented from scratch using NumPy and Pandas only"
üß† Artificial Neural Network (ANN) ‚Äî From Scratch

Mini-Batch GD | Adam Optimizer | Dropout | Full Forward + Backprop | Visualizations

This repository contains a complete scratch implementation of a Deep Neural Network (DNN) trained on a synthetic dataset.
Everything is implemented from first principles‚Äîno TensorFlow, no PyTorch.


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

üìå What This Project Covers

Forward propagation

Backpropagation

Parameter initialization

Activation functions

Mini-Batch Gradient Descent

Adam Optimizer

Dropout Regularization

Loss computation

Gradient updates

Training loop

Visualizations to build intuition

Full mathematical explanations



-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

üèóÔ∏è 1. Architecture Overview

A general L-layer deep neural network:

$$a^{[0]}$$ = X

For each layer :

$$z^{[l]}$$ = $$W^{[l]} a^{[l-1]} + b^{[l]}$$

$$a^{[l]}$$= $$\text{Activation}(z^{[l]})$$

Final output depends on task:

Classification ‚Üí sigmoid/softmax

Regression ‚Üí linear



-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

‚öôÔ∏è 2. Forward Propagation

Supported activations:

ReLU

TanH

Sigmoid


$$\text{ReLU}(z)$$=$$\max(0,z)$$

$$\tanh(z)$$=$$\frac{e^z - e^{-z}}{e^z+e^{-z}}$$


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

üîÅ 3. Backpropagation

Derived using chain rule:

Gradients

$$dZ^{[l]}$$ = $$dA^{[l]} \odot g'(Z^{[l]})$$

$$dW^{[l]}$4 = $$\frac{1}{m} dZ^{[l]} A^{[l-1]T}$$

$4db^{[l]}$$ = $$\frac{1}{m} \sum dZ^{[l]}$$

$$dA^{[l-1]}$$ = $$W^{[l]T} dZ^{[l]}$$


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

üìâ 4. Loss Function

For binary classification:

$$\mathcal{L}$$ = $$-\frac{1}{m} \sum (y\log\hat{y} + (1-y)\log(1-\hat{y}))$$


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

üß© 5. Mini-Batch Gradient Descent

Dataset is split into batches of size batch_size:

X = $$[X^{(1)}, X^{(2)}, \dots]$$

Benefits:

Faster iterations

Smoother convergence

Less memory usage



-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

üöÄ 6. Adam Optimizer (From Scratch)

Adam mixes Momentum + RMSProp:

1. Update first moment:

$$v_t$$ = $$\beta_1 v_{t-1} + (1-\beta_1) g_t$$

2. Update second moment:

$$s_t$$ = $$\beta_2 s_{t-1} + (1-\beta_2) g_t^2$$

3. Bias correction:

$$\hat{v}_t$$ = $$\frac{v_t}{1-\beta_1^t}$$

$$\hat{s}_t$$ = $$\frac{s_t}{1-\beta_2^t} ÓÄÅ$$

4. Parameter update:

$$\theta$$ = $$\theta - \alpha \frac{\hat{v}_t}{\sqrt{\hat{s}_t}+\epsilon}$$


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

üß® 7. Dropout Regularization

To avoid overfitting:

$$D^{[l]}$$ = $$( \text{rand}(A^{[l]}) < keep\_prob )$$

$$A^{[l]}$$ = $$\frac{A^{[l]} \odot D^{[l]}}{keep\_prob}$$

During inference ‚Üí dropout disabled.


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

üì¶ 8. Caching System

During forward pass you store:

$$z^{[l]}$$

$$a^{[l]}$$

dropout masks

parameters


Cache is essential for computing backward propagation efficiently.


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

üîç 9. Visualizations Included

This repo provides intuitive plots:

üéØ Loss curve , Acuuracy curve

Visualizing convergence.
üéØ Neural Network Layers as Feature Transformers (2D ‚Üí 3D ‚Üí 2D)

üéØ Activation functions

ReLU, TanH, Sigmoid plots.

üéØ Decision boundary

Shows how the neural network separates classes.

üéØ Layer activation

to show neuron firing patterns.

üéØ Dropout Mask Effect

üéØ Mini-Batch Gradient Descent Visualization

üéØ Adam Optimizer m & v Moment Visualization

üéØ Adam vs RMSProp vs SGD comparison plots

Understanding optimizer behavior.

üéØ Learning rate effect visualization

Training curves for different .

üéØ Backpropagation Gradient Flow (Vanishing/Exploding)

üéØ Training Loss Curve (Typical Deep Network)

üéØ Cross-Entropy Loss Surface Visualization

üéØ Weight Updates Over Iterations Visualization


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

üß™ 10. Dataset

Synthetic Dataset


All features scaled before training.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# üìÅ 12. Repository Structure
‚îú‚îÄ‚îÄ ann_scratch.ipynb
‚îú‚îÄ‚îÄ visualizations/
‚îÇ   ‚îú‚îÄ‚îÄ loss_curve.png
‚îÇ   ‚îú‚îÄ‚îÄ Neural Network Layers as Feature Transformers (2D ‚Üí 3D ‚Üí 2D).png
‚îÇ   ‚îú‚îÄ‚îÄ decision_boundary.png
‚îÇ   ‚îú‚îÄ‚îÄ optimizers_compare.png
‚îÇ   ‚îú‚îÄ‚îÄ Dropout Mask Effect Visualization.png
‚îî‚îÄ‚îÄ README.md
