{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca05cdb-157e-433e-b8eb-aed3a0b34e01",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks and its Applications:\n",
    "Artificial Neural Networks (ANNs) are computer systems designed to mimic how the human brain processes information. Just like the brain uses neurons to process data and make decisions, ANNs use artificial neurons to analyze data, identify patterns and make predictions. These networks consist of layers of interconnected neurons that work together to solve complex problems. The key idea is that ANNs can \"learn\" from the data they process, just as our brain learns from experience. They are used in various applications from recognizing images to making personalized recommendations. In this article, we will see more about ANNs, how they function and other core concepts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39bcfa80-8122-4e3b-ac22-a928b42da232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First,We will import all the important libraries:-\n",
    "# ==========================================================================================================================================================================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "np.random.seed(42)\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1868171-c49e-4e5c-95c0-7463d5cf5a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (800, 2) (800, 1) (200, 2) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "X, y = make_moons(n_samples=1000, noise=0.15, random_state=42)\n",
    "y = y.reshape(-1, 1)  # shape (n,1)\n",
    "# ==========================================================================================================================================================================================\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# ==========================================================================================================================================================================================\n",
    "# Standardize features (important for NN training)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# ==========================================================================================================================================================================================\n",
    "print(\"Shapes:\", X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10ef29dd-0cba-4061-8915-b19434ab2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "# ==========================================================================================================================================================================================\n",
    "def sigmoid_derivative(a):  # a = sigmoid(z)\n",
    "    return a * (1 - a)\n",
    "# ==========================================================================================================================================================================================\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "# ==========================================================================================================================================================================================\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "# ==========================================================================================================================================================================================\n",
    "def binary_accuracy(y_true, y_pred_probs, threshold=0.5):\n",
    "    preds = (y_pred_probs >= threshold).astype(int)\n",
    "    return (preds == y_true).mean()\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ae3d0c2-f0a3-40b4-b134-57b8533467ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape: (2, 16) b1: (1, 16) W2: (16, 1) b2: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "def init_params(n_input, n_hidden, n_output=1, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # He initialization for layers followed by ReLU\n",
    "    W1 = rng.randn(n_input, n_hidden) * np.sqrt(2. / n_input)\n",
    "    b1 = np.zeros((1, n_hidden))\n",
    "    # For output (sigmoid) small random init\n",
    "    W2 = rng.randn(n_hidden, n_output) * np.sqrt(2. / n_hidden)\n",
    "    b2 = np.zeros((1, n_output))\n",
    "    return W1, b1, W2, b2\n",
    "# ==========================================================================================================================================================================================\n",
    "# Example sizes\n",
    "n_input = X_train.shape[1]   # 2\n",
    "n_hidden = 16                # you can change\n",
    "W1, b1, W2, b2 = init_params(n_input, n_hidden)\n",
    "print(\"W1 shape:\", W1.shape, \"b1:\", b1.shape, \"W2:\", W2.shape, \"b2:\", b2.shape)\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f51833e-c11b-459d-9ace-6f54ca7c7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    # X: (m, n_input)\n",
    "    Z1 = X.dot(W1) + b1      # (m, n_hidden)\n",
    "    A1 = relu(Z1)            # (m, n_hidden)\n",
    "    Z2 = A1.dot(W2) + b2     # (m, 1)\n",
    "    A2 = sigmoid(Z2)         # (m, 1) — output probabilities\n",
    "    cache = (Z1, A1, Z2, A2)\n",
    "    return A2, cache\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34c29d4e-d5ce-4408-9263-f82eeeaae9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "def compute_loss(Y, Y_hat):\n",
    "    # Y, Y_hat: shape (m,1)\n",
    "    m = Y.shape[0]\n",
    "    eps = 1e-12\n",
    "    loss = - (Y * np.log(Y_hat + eps) + (1 - Y) * np.log(1 - Y_hat + eps))\n",
    "    return loss.mean()\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e77dd0ee-bd70-4dbe-bbc9-9d37ae89b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "def backward(X, Y, cache, W2):\n",
    "    # cache = (Z1, A1, Z2, A2)\n",
    "    Z1, A1, Z2, A2 = cache\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # Output layer gradients\n",
    "    dZ2 = A2 - Y                      # (m,1) for BCE with sigmoid\n",
    "    dW2 = (A1.T.dot(dZ2)) / m         # (n_hidden, 1)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m  # (1,1)\n",
    "\n",
    "    # Hidden layer gradients\n",
    "    dA1 = dZ2.dot(W2.T)               # (m, n_hidden)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)   # (m, n_hidden)\n",
    "    dW1 = (X.T.dot(dZ1)) / m          # (n_input, n_hidden)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # (1, n_hidden)\n",
    "\n",
    "    grads = (dW1, db1, dW2, db2)\n",
    "    return grads\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4554e6c4-26b5-4404-a576-7fd19b616907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "def update_params(W1, b1, W2, b2, grads, lr):\n",
    "    dW1, db1, dW2, db2 = grads\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    return W1, b1, W2, b2\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d21c78a1-c7a8-4766-a9b3-6c1fa024f0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 — Train loss: 0.6799, Train acc: 0.4400 — Test loss: 0.3098, Test acc: 0.8650\n",
      "Epoch  200 — Train loss: 0.0956, Train acc: 0.9762 — Test loss: 0.0716, Test acc: 0.9850\n",
      "Epoch  400 — Train loss: 0.0455, Train acc: 0.9900 — Test loss: 0.0292, Test acc: 1.0000\n",
      "Epoch  600 — Train loss: 0.0335, Train acc: 0.9938 — Test loss: 0.0187, Test acc: 1.0000\n",
      "Epoch  800 — Train loss: 0.0282, Train acc: 0.9925 — Test loss: 0.0141, Test acc: 1.0000\n",
      "Epoch 1000 — Train loss: 0.0249, Train acc: 0.9950 — Test loss: 0.0116, Test acc: 1.0000\n",
      "Epoch 1200 — Train loss: 0.0226, Train acc: 0.9950 — Test loss: 0.0099, Test acc: 1.0000\n",
      "Epoch 1400 — Train loss: 0.0210, Train acc: 0.9950 — Test loss: 0.0088, Test acc: 1.0000\n",
      "Epoch 1600 — Train loss: 0.0198, Train acc: 0.9950 — Test loss: 0.0079, Test acc: 1.0000\n",
      "Epoch 1800 — Train loss: 0.0189, Train acc: 0.9950 — Test loss: 0.0073, Test acc: 1.0000\n",
      "Epoch 2000 — Train loss: 0.0182, Train acc: 0.9950 — Test loss: 0.0067, Test acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "# Hyperparameters\n",
    "n_hidden = 32\n",
    "lr = 0.5\n",
    "n_epochs = 2000\n",
    "print_every = 200\n",
    "# ==========================================================================================================================================================================================\n",
    "# Initialize\n",
    "W1, b1, W2, b2 = init_params(n_input, n_hidden, n_output=1, seed=42)\n",
    "# ==========================================================================================================================================================================================\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "# ==========================================================================================================================================================================================\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Forward\n",
    "    Y_hat_train, cache = forward(X_train, W1, b1, W2, b2)\n",
    "    loss = compute_loss(y_train, Y_hat_train)\n",
    "    acc = binary_accuracy(y_train, Y_hat_train)\n",
    "\n",
    "    # Backward\n",
    "    grads = backward(X_train, y_train, cache, W2)\n",
    "\n",
    "    # Update\n",
    "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, grads, lr)\n",
    "\n",
    "    # Track\n",
    "    train_losses.append(loss)\n",
    "    train_accs.append(acc)\n",
    "    \n",
    "    # Evaluate on test set periodically\n",
    "    if epoch % 10 == 0:\n",
    "        Y_hat_test, _ = forward(X_test, W1, b1, W2, b2)\n",
    "        test_loss = compute_loss(y_test, Y_hat_test)\n",
    "        test_acc = binary_accuracy(y_test, Y_hat_test)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "    if epoch % print_every == 0 or epoch == 1:\n",
    "        Y_hat_test, _ = forward(X_test, W1, b1, W2, b2)\n",
    "        test_loss = compute_loss(y_test, Y_hat_test)\n",
    "        test_acc = binary_accuracy(y_test, Y_hat_test)\n",
    "        print(f\"Epoch {epoch:4d} — Train loss: {loss:.4f}, Train acc: {acc:.4f} — Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1293b79-097c-4c7d-82d9-3b1d32d8e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "# Utility functions\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    return W1, b1, W2, b2\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bba68b71-df2d-4675-a406-e02d03490b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "# Forward Propagation\n",
    "\n",
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(W1, X) + b1     # (n_h, m)\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2    # (1, m)\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = (Z1, A1, Z2, A2)\n",
    "    return A2, cache\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4af40e5-0b33-4a72-961c-63b243f7cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "# Compute Loss\n",
    "def compute_loss(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    loss = -np.mean(Y * np.log(A2 + 1e-9) + (1 - Y) * np.log(1 - A2 + 1e-9))\n",
    "    return loss\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7bd42a5-8598-4d7a-aa86-e99752f6e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "# Adam Optimizer Initialization\n",
    "\n",
    "def init_adam(W1, b1, W2, b2):\n",
    "    v = {\n",
    "        \"dW1\": np.zeros_like(W1),\n",
    "        \"db1\": np.zeros_like(b1),\n",
    "        \"dW2\": np.zeros_like(W2),\n",
    "        \"db2\": np.zeros_like(b2)\n",
    "    }\n",
    "    s = {\n",
    "        \"dW1\": np.zeros_like(W1),\n",
    "        \"db1\": np.zeros_like(b1),\n",
    "        \"dW2\": np.zeros_like(W2),\n",
    "        \"db2\": np.zeros_like(b2)\n",
    "    }\n",
    "    return v, s\n",
    "# ==========================================================================================================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10f62d1c-ef09-475b-8e67-c81a103e6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "# Update parameters using Adam\n",
    "\n",
    "def adam_update(params, grads, v, s, t, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "    W1, b1, W2, b2 = params\n",
    "    dW1, db1, dW2, db2 = grads\n",
    "    \n",
    "    # Moving averages of gradients\n",
    "    for key, grad in zip([\"dW1\", \"db1\", \"dW2\", \"db2\"],\n",
    "                         [dW1, db1, dW2, db2]):\n",
    "        v[key] = beta1 * v[key] + (1 - beta1) * grad\n",
    "        s[key] = beta2 * s[key] + (1 - beta2) * (grad ** 2)\n",
    "    \n",
    "    # Bias correction\n",
    "    v_corrected = {k: v[k] / (1 - beta1**t) for k in v}\n",
    "    s_corrected = {k: s[k] / (1 - beta2**t) for k in s}\n",
    "    \n",
    "    # Parameter updates\n",
    "    W1 -= lr * v_corrected[\"dW1\"] / (np.sqrt(s_corrected[\"dW1\"]) + eps)\n",
    "    b1 -= lr * v_corrected[\"db1\"] / (np.sqrt(s_corrected[\"db1\"]) + eps)\n",
    "    W2 -= lr * v_corrected[\"dW2\"] / (np.sqrt(s_corrected[\"dW2\"]) + eps)\n",
    "    b2 -= lr * v_corrected[\"db2\"] / (np.sqrt(s_corrected[\"db2\"]) + eps)\n",
    "    \n",
    "    return (W1, b1, W2, b2), v, s\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e83e3cb-b47f-446d-a4db-ee5623c0fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================================================================================================================\n",
    "# Training Loop with Mini-Batch Gradient Descent + Adam\n",
    "\n",
    "def train(X, Y, n_h=8, epochs=1000, batch_size=64, lr=0.001):\n",
    "    n_x = X.shape[0]\n",
    "    n_y = 1\n",
    "    \n",
    "    # Initialize parameters\n",
    "    W1, b1, W2, b2 = initialize_parameters(n_x, n_h, n_y)\n",
    "    v, s = init_adam(W1, b1, W2, b2)\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    for t in range(1, epochs + 1):\n",
    "        permutation = np.random.permutation(m)\n",
    "        X_shuffled = X[:, permutation]\n",
    "        Y_shuffled = Y[:, permutation]\n",
    "        \n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[:, i:i+batch_size]\n",
    "            Y_batch = Y_shuffled[:, i:i+batch_size]\n",
    "            \n",
    "            # Forward + Backward + Adam update\n",
    "            A2, cache = forward_pass(X_batch, W1, b1, W2, b2)\n",
    "            grads = backward_pass(X_batch, Y_batch, cache, W2)\n",
    "            (W1, b1, W2, b2), v, s = adam_update(\n",
    "                (W1, b1, W2, b2), grads, v, s, t, lr=lr\n",
    "            )\n",
    "        \n",
    "        # Print loss\n",
    "        if t % 100 == 0:\n",
    "            loss = compute_loss(A2, Y_batch)\n",
    "            print(f\"Epoch {t}, Loss = {loss:.4f}\")\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7976d87-3114-4633-bfae-85a465526f6f",
   "metadata": {},
   "source": [
    "# Now,Scartch implementation of Deep Neural Network(DNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2468666-b508-4a26-8d04-678b15d66426",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning:-\n",
    "Deep Learning is transforming the way machines understand, learn and interact with complex data. Deep learning mimics neural networks of the human brain, it enables computers to autonomously uncover patterns and make informed decisions from vast amounts of unstructured data.\n",
    "# How Deep Learning Works?\n",
    "Neural network consists of layers of interconnected nodes or neurons that collaborate to process input data. In a fully connected deep neural network data flows through multiple layers where each neuron performs nonlinear transformations, allowing the model to learn intricate representations of the data.\n",
    "\n",
    "In a deep neural network the input layer receives data which passes through hidden layers that transform the data using nonlinear functions. The final output layer generates the model’s prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9cb8b2e-dd9d-44d3-87d8-937de6cfca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================================================================================================================================================\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b52be8a2-155f-4ba9-bcf7-cde3260534e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities: activations + grads\n",
    "# ==========================================================================================================================================================================================\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "# ==========================================================================================================================================================================================\n",
    "def sigmoid_backward(dA, Z):\n",
    "    A = sigmoid(Z)\n",
    "    return dA * A * (1 - A)\n",
    "# ==========================================================================================================================================================================================\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "# ==========================================================================================================================================================================================\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "# ==========================================================================================================================================================================================\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "# ==========================================================================================================================================================================================\n",
    "def tanh_backward(dA, Z):\n",
    "    A = np.tanh(Z)\n",
    "    return dA * (1 - A ** 2)\n",
    "# ==========================================================================================================================================================================================\n",
    "def softmax(Z):\n",
    "    Z_shift = Z - np.max(Z, axis=0, keepdims=True)\n",
    "    expZ = np.exp(Z_shift)\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ca6854b-37b4-402c-ab7f-b1b657283da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization:-\n",
    "# ==========================================================================================================================================================================================\n",
    "def initialize_parameters_deep(layer_dims, seed=42):\n",
    "    \"\"\"\n",
    "    Here,We have initialized-\n",
    "    layer_dims: list of layer sizes, e.g. [n_x, 128, 64, n_y]\n",
    "    returns params dict with W1,b1,... Wl,bl\n",
    "    Used He init for ReLU/tanh, Xavier for sigmoid/tanh optionally.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    params = {}\n",
    "    L = len(layer_dims) - 1\n",
    "    for l in range(1, L + 1):\n",
    "        n_l = layer_dims[l]\n",
    "        n_prev = layer_dims[l - 1]\n",
    "        # Imp:-He initialization (works well generally for ReLU)\n",
    "        params['W' + str(l)] = np.random.randn(n_l, n_prev) * np.sqrt(2. / n_prev)\n",
    "        params['b' + str(l)] = np.zeros((n_l, 1))\n",
    "    return params\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66f3159d-b746-4569-9cb7-4003cc4d6af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward (linear -> activation) with dropout\n",
    "# ==========================================================================================================================================================================================\n",
    "def linear_forward(A_prev, W, b):\n",
    "    Z = W.dot(A_prev) + b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache\n",
    "# ==========================================================================================================================================================================================\n",
    "def activation_forward(A_prev, W, b, activation, keep_prob=1.0):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == 'relu':\n",
    "        A = relu(Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == 'tanh':\n",
    "        A = tanh(Z)\n",
    "    elif activation == 'softmax':\n",
    "        A = softmax(Z)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation\")\n",
    "    D = None\n",
    "    if keep_prob < 1.0:\n",
    "        D = (np.random.rand(*A.shape) < keep_prob).astype(float)\n",
    "        A = A * D\n",
    "        A = A / keep_prob\n",
    "    cache = (linear_cache, Z, D)\n",
    "    return A, cache\n",
    "# ==========================================================================================================================================================================================\n",
    "def forward_model(X, params, activations, keep_probs=None):\n",
    "    \"\"\"\n",
    "    # Explanation of the hyperparameters we  used:-\n",
    "    X: input data of shape (n_x, m)\n",
    "    params: dict\n",
    "    activations: list of activation names for layers [act1, act2, ..., actL]\n",
    "                 last activation should be 'softmax' for multiclass or 'sigmoid' for binary\n",
    "    keep_probs: list of keep_prob values for dropout for each layer (same length as activations),\n",
    "                or None (defaults to 1.0 for all)\n",
    "    Returns:\n",
    "      AL: output activation\n",
    "      caches: list of caches\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(activations)\n",
    "    if keep_probs is None:\n",
    "        keep_probs = [1.0] * L\n",
    "    for l in range(1, L + 1):\n",
    "        W = params['W' + str(l)]\n",
    "        b = params['b' + str(l)]\n",
    "        act = activations[l - 1]\n",
    "        keep_prob = keep_probs[l - 1]\n",
    "        A, cache = activation_forward(A, W, b, act, keep_prob)\n",
    "        caches.append(cache)\n",
    "    return A, caches\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9f2b2fc-240c-4a5f-a329-ebe110681596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "# ==========================================================================================================================================================================================\n",
    "def compute_cost(AL, Y, params=None, lambda_l2=0.0):\n",
    "    \"\"\"\n",
    "    # Explanation of the hyperparameters we  used:-\n",
    "    AL: predictions (n_y, m)\n",
    "    Y: true labels (n_y, m) one-hot for multiclass, or shape (1,m) for binary\n",
    "    lambda_l2: L2 reg coefficient\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    eps = 1e-12\n",
    "    if AL.shape[0] == 1:\n",
    "        # binary\n",
    "        cost = -np.sum(Y * np.log(AL + eps) + (1 - Y) * np.log(1 - AL + eps)) / m\n",
    "    else:\n",
    "        # multiclass\n",
    "        cost = -np.sum(Y * np.log(AL + eps)) / m\n",
    "    if lambda_l2 and params is not None:\n",
    "        L = len([k for k in params.keys() if k.startswith('W')])\n",
    "        l2_sum = 0\n",
    "        for l in range(1, L + 1):\n",
    "            l2_sum += np.sum(np.square(params['W' + str(l)]))\n",
    "        cost += (lambda_l2 / (2 * m)) * l2_sum\n",
    "    return cost\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c84a345-4635-4dd9-836e-136b0de0e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward functions\n",
    "# ==========================================================================================================================================================================================\n",
    "def linear_backward(dZ, linear_cache):\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1 / m) * dZ.dot(A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = W.T.dot(dZ)\n",
    "    return dA_prev, dW, db\n",
    "# ==========================================================================================================================================================================================\n",
    "def activation_backward(dA, cache, activation, keep_prob=1.0):\n",
    "    linear_cache, Z, D = cache\n",
    "    if D is not None:\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_prob\n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backward(dA, Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, Z)\n",
    "    elif activation == 'tanh':\n",
    "        dZ = tanh_backward(dA, Z)\n",
    "    elif activation == 'softmax':\n",
    "        # For softmax with cross-entropy, dZ = AL - Y is computed outside for efficiency\n",
    "        dZ = dA\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation\")\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db\n",
    "# ==========================================================================================================================================================================================\n",
    "def backward_model(AL, Y, params, caches, activations, keep_probs=None, lambda_l2=0.0):\n",
    "    \"\"\"\n",
    "    # Explanation of the hyperparameters we  used:-\n",
    "    AL: predictions (n_y, m)\n",
    "    Y: true labels (n_y, m)\n",
    "    caches: list of caches from forward\n",
    "    activations: list of activations\n",
    "    Returns grads dict with dWl, dbl, ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    if keep_probs is None:\n",
    "        keep_probs = [1.0] * L\n",
    "\n",
    "    # Initialize dA for last layer\n",
    "    if activations[-1] == 'softmax':\n",
    "        # dZ = AL - Y (shape (n_y, m))\n",
    "        dA = AL - Y\n",
    "    elif activations[-1] == 'sigmoid' and AL.shape[0] == 1:\n",
    "        dA = -(np.divide(Y, AL + 1e-12) - np.divide(1 - Y, 1 - AL + 1e-12))\n",
    "        # Alternatively simpler: dZ = AL - Y gives same for BCE with sigmoid\n",
    "        dA = AL - Y\n",
    "    else:\n",
    "        dA = AL - Y  # fallback for regression-like or other combos\n",
    "\n",
    "    # Backprop through layers\n",
    "    current_dA = dA\n",
    "    for l in reversed(range(1, L + 1)):\n",
    "        cache = caches[l - 1]\n",
    "        act = activations[l - 1]\n",
    "        keep_prob = keep_probs[l - 1]\n",
    "        dA_prev, dW, db = activation_backward(current_dA, cache, act, keep_prob)\n",
    "        # Add L2 regularization to dW\n",
    "        if lambda_l2:\n",
    "            dW += (lambda_l2 / m) * params['W' + str(l)]\n",
    "        grads['dW' + str(l)] = dW\n",
    "        grads['db' + str(l)] = db\n",
    "        current_dA = dA_prev\n",
    "    return grads\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0cdc19fd-5d45-4b27-8149-9a7275f2421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer helpers\n",
    "# ==========================================================================================================================================================================================\n",
    "def initialize_adam_params(params):\n",
    "    L = len([k for k in params.keys() if k.startswith('W')])\n",
    "    v = {}\n",
    "    s = {}\n",
    "    for l in range(1, L + 1):\n",
    "        v['dW' + str(l)] = np.zeros_like(params['W' + str(l)])\n",
    "        v['db' + str(l)] = np.zeros_like(params['b' + str(l)])\n",
    "        s['dW' + str(l)] = np.zeros_like(params['W' + str(l)])\n",
    "        s['db' + str(l)] = np.zeros_like(params['b' + str(l)])\n",
    "    return v, s\n",
    "# ==========================================================================================================================================================================================\n",
    "def update_parameters_adam(params, grads, v, s, t, learning_rate=0.001,\n",
    "                           beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    L = len([k for k in params.keys() if k.startswith('W')])\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    for l in range(1, L + 1):\n",
    "        # moving averages\n",
    "        v['dW' + str(l)] = beta1 * v['dW' + str(l)] + (1 - beta1) * grads['dW' + str(l)]\n",
    "        v['db' + str(l)] = beta1 * v['db' + str(l)] + (1 - beta1) * grads['db' + str(l)]\n",
    "        s['dW' + str(l)] = beta2 * s['dW' + str(l)] + (1 - beta2) * (grads['dW' + str(l)] ** 2)\n",
    "        s['db' + str(l)] = beta2 * s['db' + str(l)] + (1 - beta2) * (grads['db' + str(l)] ** 2)\n",
    "\n",
    "        # bias correction\n",
    "        v_corrected['dW' + str(l)] = v['dW' + str(l)] / (1 - beta1 ** t)\n",
    "        v_corrected['db' + str(l)] = v['db' + str(l)] / (1 - beta1 ** t)\n",
    "        s_corrected['dW' + str(l)] = s['dW' + str(l)] / (1 - beta2 ** t)\n",
    "        s_corrected['db' + str(l)] = s['db' + str(l)] / (1 - beta2 ** t)\n",
    "\n",
    "        # update params\n",
    "        params['W' + str(l)] -= learning_rate * (v_corrected['dW' + str(l)] / (np.sqrt(s_corrected['dW' + str(l)]) + epsilon))\n",
    "        params['b' + str(l)] -= learning_rate * (v_corrected['db' + str(l)] / (np.sqrt(s_corrected['db' + str(l)]) + epsilon))\n",
    "    return params, v, s\n",
    "\n",
    "# ==========================================================================================================================================================================================\n",
    "# Mini-batch utils\n",
    "# ==========================================================================================================================================================================================\n",
    "def random_mini_batches(X, Y, mini_batch_size=64, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]\n",
    "    permutation = np.random.permutation(m)\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "    mini_batches = []\n",
    "    num_complete_minibatches = m // mini_batch_size\n",
    "    for k in range(num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    return mini_batches\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "820cf3ce-5796-40fa-a245-4707c9e0fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training:-\n",
    "# ==========================================================================================================================================================================================\n",
    "def model_train(X, Y, layer_dims, activations,\n",
    "                keep_probs=None,\n",
    "                learning_rate=0.001, num_epochs=1000,\n",
    "                mini_batch_size=64, print_cost=True,\n",
    "                lambda_l2=0.0, seed=42):\n",
    "    \"\"\"\n",
    "    # Explanation of the hyperparameters we  used:-\n",
    "    X: (n_x, m)\n",
    "    Y: (n_y, m) one-hot for multiclass or (1,m) for binary\n",
    "    layer_dims: list dims e.g. [n_x, 128, 64, n_y]\n",
    "    activations: list len L of activation names for each layer (last must match output)\n",
    "    keep_probs: list len L of dropout keep probabilities (1.0 => no dropout)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    params = initialize_parameters_deep(layer_dims, seed)\n",
    "    v, s = initialize_adam_params(params)\n",
    "    t = 0\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    if keep_probs is None:\n",
    "        keep_probs = [1.0] * len(activations)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed=epoch)\n",
    "        epoch_cost = 0\n",
    "        for minibatch_X, minibatch_Y in minibatches:\n",
    "            # forward\n",
    "            AL, caches = forward_model(minibatch_X, params, activations, keep_probs)\n",
    "            # cost\n",
    "            cost = compute_cost(AL, minibatch_Y, params, lambda_l2)\n",
    "            epoch_cost += cost * minibatch_X.shape[1] / m  # weighted average\n",
    "\n",
    "            # backward\n",
    "            grads = backward_model(AL, minibatch_Y, params, caches, activations, keep_probs, lambda_l2)\n",
    "            # update t and params via Adam\n",
    "            t += 1\n",
    "            params, v, s = update_parameters_adam(params, grads, v, s, t, learning_rate)\n",
    "        costs.append(epoch_cost)\n",
    "\n",
    "        if print_cost and (epoch % max(1, num_epochs // 10) == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch}/{num_epochs} — cost: {epoch_cost:.6f}\")\n",
    "    return params, costs\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2822cd69-1472-4e4b-a828-119ffc53d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions / Accuracy on our trained model\n",
    "# ==========================================================================================================================================================================================\n",
    "def predict(X, params, activations):\n",
    "    AL, _ = forward_model(X, params, activations, keep_probs=[1.0]*len(activations))\n",
    "    if AL.shape[0] == 1:\n",
    "        preds = (AL > 0.5).astype(int)\n",
    "        return preds\n",
    "    else:\n",
    "        preds = np.argmax(AL, axis=0)\n",
    "        return preds\n",
    "\n",
    "def accuracy_score(Y_true_labels, Y_pred_labels):\n",
    "    return np.mean(Y_true_labels == Y_pred_labels)\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72d9cb31-be06-4a99-baa2-39d3f2812ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 — cost: 2.060581\n",
      "Epoch 10/100 — cost: 0.055774\n",
      "Epoch 20/100 — cost: 0.015778\n",
      "Epoch 30/100 — cost: 0.008712\n",
      "Epoch 40/100 — cost: 0.006447\n",
      "Epoch 50/100 — cost: 0.005396\n",
      "Epoch 60/100 — cost: 0.004858\n",
      "Epoch 70/100 — cost: 0.004534\n",
      "Epoch 80/100 — cost: 0.004318\n",
      "Epoch 90/100 — cost: 0.004166\n",
      "Epoch 100/100 — cost: 0.004050\n",
      "Train accuracy: 1.0000, Test accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "# Here,We used: digits dataset\n",
    "# ==========================================================================================================================================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    digits = load_digits()\n",
    "    X = digits.data  # (n_samples, n_features) = (1797, 64)\n",
    "    y = digits.target.reshape(-1, 1)  # labels 0..9\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train).T  # shape (n_x, m)\n",
    "    X_test = scaler.transform(X_test).T\n",
    "\n",
    "    # One-hot encode labels\n",
    "    encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "    Y_train = encoder.fit_transform(y_train).T  # shape (n_y, m)\n",
    "    Y_test = encoder.transform(y_test).T\n",
    "\n",
    "    # Network configuration\n",
    "    n_x = X_train.shape[0]\n",
    "    n_y = Y_train.shape[0]\n",
    "    layer_dims = [n_x, 128, 64, n_y]\n",
    "    activations = ['relu', 'relu', 'softmax']  # last softmax for multiclass\n",
    "    keep_probs = [1.0, 1.0, 1.0]  # no dropout by default\n",
    "\n",
    "    # Train\n",
    "    params, costs = model_train(X_train, Y_train,\n",
    "                                layer_dims, activations,\n",
    "                                keep_probs=keep_probs,\n",
    "                                learning_rate=0.001,\n",
    "                                num_epochs=100,\n",
    "                                mini_batch_size=64,\n",
    "                                print_cost=True,\n",
    "                                lambda_l2=0.001)\n",
    "    # ==========================================================================================================================================================================================\n",
    "    # Predict & evaluate\n",
    "    y_pred_train = predict(X_train, params, activations)\n",
    "    y_true_train = np.argmax(Y_train, axis=0)\n",
    "    train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "\n",
    "    y_pred_test = predict(X_test, params, activations)\n",
    "    y_true_test = np.argmax(Y_test, axis=0)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_test)\n",
    "\n",
    "    print(f\"Train accuracy: {train_acc:.4f}, Test accuracy: {test_acc:.4f}\")\n",
    "# =========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d62b14-0127-4330-88dd-20ac7e589be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
